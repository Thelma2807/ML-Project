# %% [markdown]
# # Imports de base

# %%
# --- 0. Imports & config ---
import os, math, warnings
from pathlib import Path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GroupShuffleSplit, train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, RocCurveDisplay

warnings.filterwarnings("ignore")
plt.rcParams["figure.figsize"] = (10,6)

# Dossier du notebook (mettre "." si le notebook est dans le m√™me dossier que les CSV)
BASE = Path(".")

# Lister les CSV disponibles dans le dossier (‚ö†Ô∏è on liste un dossier, pas un fichier)
csv_files = sorted(p.name for p in BASE.glob("*.csv"))
csv_files


# %% [markdown]
#  # I - Analyse descriptive des donn√©es

# %%
# Colonnes communes aux IMU (certains fichiers n'ont pas tout)
COMMON = ['time','accX','accY','accZ','gyroX','gyroY','gyroZ','magX','magY','magZ','name']

def read_common(path, nrows=None):
    """Charge un CSV et ne garde que les colonnes IMU connues + source."""
    df = pd.read_csv(path, nrows=nrows)
    keep = [c for c in COMMON if c in df.columns]
    df = df[keep].copy()
    for c in df.columns:
        if c not in ('time','name'):
            df[c] = pd.to_numeric(df[c], errors='coerce')
    df['source'] = Path(path).name
    return df

def add_norms(df):
    """Ajoute des normes acc/gyro/mag si possible."""
    for name, cols in {
        'acc_norm':  ['accX','accY','accZ'],
        'gyro_norm': ['gyroX','gyroY','gyroZ'],
        'mag_norm':  ['magX','magY','magZ'],
    }.items():
        have = [c for c in cols if c in df.columns]
        if len(have)==3:
            df[name] = np.sqrt((df[have]**2).sum(axis=1))
    return df

# √âchantillon l√©ger pour EDA
eda_frames = []
for f in csv_files:
    if f.lower().startswith("quaternion"):     # on exclut les quaternions pour l'instant
        continue
    try:
        eda_frames.append(read_common(BASE/f, nrows=5000))
    except Exception as e:
        print(f"[WARN] {f}: {e}")

eda = pd.concat(eda_frames, ignore_index=True, sort=False)
eda = add_norms(eda)

# Taille & colonnes par fichier
sizes = (eda.groupby('source')['source']
             .size()
             .rename('rows')
             .to_frame())
sizes['cols_present'] = eda.groupby('source').apply(lambda g: list(g.columns))
sizes


# %%
# Distributions (acc_norm / gyro_norm / mag_norm) par fichier
for col in ['acc_norm','gyro_norm','mag_norm']:
    if col in eda:
        plt.figure()
        for s, g in eda.groupby('source'):
            g[col].dropna().sample(min(2000, len(g)), random_state=0).plot(kind='hist', bins=60, alpha=0.35, label=s)
        plt.title(f"Distribution de {col}")
        plt.xlabel(col); plt.ylabel("comptes"); plt.legend()
        plt.show()


# %%
# Estimation robuste de la fr√©quence d'√©chantillonnage par fichier
def estimate_hz(df):
    if 'time' not in df:
        return np.nan
    # 1) Essai float (timestamps en secondes)
    ts = pd.to_numeric(df['time'], errors='coerce')
    if ts.notna().sum() >= 3:
        d = np.diff(ts.dropna().values)
        d = d[d>0]
        if len(d): 
            med = np.median(d)
            return round(1.0/med, 3) if med>0 else np.nan
    # 2) Essai datetime
    t = pd.to_datetime(df['time'], errors='coerce', utc=True, infer_datetime_format=True)
    if t.notna().sum() >= 3:
        d = np.diff(t.dropna().astype('int64').values/1e9)
        d = d[d>0]
        if len(d):
            med = np.median(d)
            return round(1.0/med, 3) if med>0 else np.nan
    return np.nan

hz = eda.groupby('source').apply(estimate_hz).rename('Hz').reset_index()
hz


# %% [markdown]
# # II - Pr√©-traitement

# %%
# Labellisation √† partir du nom de fichier
ANOMYMS = ['earthquake','hitting_arm','hitting_platform','extra_weigth','magnet']
def label_from_name(name: str) -> int:
    n = name.lower()
    return int(any(k in n for k in ANOMYMS))

# Charge tous les fichiers IMU (sauf quaternions pour l'instant) et pr√©pare les features
all_frames = []
for f in csv_files:
    if f.lower().startswith("quaternion"):
        continue
    try:
        df = read_common(BASE/f, nrows=None)   # mettre nrows=200_000 si on veux aller plus vite
        df['label'] = label_from_name(f)
        all_frames.append(df)
    except Exception as e:
        print(f"[WARN] {f}: {e}")

raw = pd.concat(all_frames, ignore_index=True, sort=False)
raw = add_norms(raw)

# Colonnes features disponibles
feature_cols = [c for c in ['accX','accY','accZ','gyroX','gyroY','gyroZ','magX','magY','magZ',
                            'acc_norm','gyro_norm','mag_norm'] if c in raw.columns]
raw = raw.dropna(subset=feature_cols, how='all').reset_index(drop=True)

# Normalisation par fichier (√©vite les biais de calibration capteur)
scalers = {}
def scale_by_source(df, cols):
    parts = []
    for s, g in df.groupby('source', sort=False):
        sc = StandardScaler()
        g = g.copy()
        g[cols] = sc.fit_transform(g[cols].fillna(method='ffill').fillna(method='bfill').fillna(0.0))
        scalers[s] = sc
        parts.append(g)
    return pd.concat(parts, ignore_index=True)

proc = scale_by_source(raw, feature_cols)
proc.head()


# %%
# Fen√™trage statistique (features sur fen√™tres glissantes)
WIN = 50          # 50 √©chantillons
OVERLAP = 0.5     # 50% de recouvrement

def window_features(df, cols, win=WIN, overlap=OVERLAP):
    step = max(1, int(win*(1-overlap)))
    N = len(df)
    rows = []
    for start in range(0, N - win + 1, step):
        end = start + win
        sl = df.iloc[start:end]
        row = {}
        for c in cols:
            s = sl[c].values
            row[f'{c}_mean'] = s.mean()
            row[f'{c}_std']  = s.std()
            row[f'{c}_min']  = s.min()
            row[f'{c}_max']  = s.max()
            row[f'{c}_rms']  = np.sqrt((s**2).mean())
        # label majorit√© dans la fen√™tre
        row['label']  = int(sl['label'].mean() >= 0.5)
        row['source'] = sl['source'].iloc[0]
        rows.append(row)
    return pd.DataFrame(rows)

blocks = []
for s, g in proc.groupby('source', sort=False):
    if len(g) >= WIN:
        blocks.append(window_features(g, feature_cols, win=WIN, overlap=OVERLAP))

dataset = pd.concat(blocks, ignore_index=True, sort=False)
dataset['label'].value_counts()


# %% [markdown]
# # III - Formalisation du probl√®me
# 
# ## 1) Contexte & but
# Nous disposons de mesures **IMU** d‚Äôun bras UR3e (acc√©l√©ration, gyroscope, magn√©tom√®tre) collect√©es lors de t√¢ches normales (screwdriving, painting, pick-and-place) et lors d‚Äô**anomalies simul√©es** (earthquake, hitting_arm, hitting_platform, extra_weigth, magnet).  
# L‚Äôobjectif op√©rationnel est une **d√©tection d‚Äôanomalies en temps r√©el** compatible TinyML : d√©clencher une alerte fiable au fil de l‚Äôeau, avec peu de calcul et une latence faible.
# 
# **Formulation ML** ‚Üí *classification binaire fen√™tre-par-fen√™tre* :  
# - **Entr√©e** : descripteurs statistiques extraits sur des segments temporels glissants des signaux IMU.  
# - **Sortie** : `y ‚àà {0,1}` avec `0 = normal`, `1 = anomalie`.
# 
# ---
# 
# ## 2) Unit√© d‚Äôobservation (fen√™trage temporel)
# Les fichiers sont √©chantillonn√©s √† diff√©rentes fr√©quences (2/10/20 Hz). Pour homog√©n√©iser l‚Äôanalyse, on travaille sur des **fen√™tres de `W` √©chantillons** (valeur par d√©faut `W = 50`).  
# Cons√©quences temporelles approximatives :
# - 20 Hz ‚Üí 50 pts ‚âà **2,5 s**  
# - 10 Hz ‚Üí 50 pts ‚âà **5 s**  
# - 2 Hz  ‚Üí 50 pts ‚âà **25 s** (fen√™tre longue, √† r√©duire si besoin pour ce cas)
# 
# > Remarque : on peut utiliser un **recouvrement** (ex. 50 %) pour augmenter la granularit√© sans augmenter les co√ªts de labellisation.
# 
# **D√©coupage en fen√™tres** (sans recouvrement pour la baseline) :  
# Si un fichier a `N` lignes, le nombre de fen√™tres est `‚åäN / W‚åã`.
# 
# ---
# 
# ## 3) Variables d‚Äôentr√©e (features)
# √Ä partir des axes `(X,Y,Z)` on calcule d‚Äôabord les **normes** :
# \[
# \text{acc\_norm}=\sqrt{accX^2+accY^2+accZ^2},\quad
# \text{gyro\_norm}=\sqrt{gyroX^2+gyroY^2+gyroZ^2},\quad
# \text{mag\_norm}=\sqrt{magX^2+magY^2+magZ^2}.
# \]
# 
# Sur chaque fen√™tre, pour chaque signal retenu (axes et/ou normes), on extrait des **statistiques robustes** :
# - moyenne, √©cart-type, min, max, RMS \(\left(\sqrt{\frac{1}{W}\sum s^2}\right)\).
# 
# > Ces features sont peu co√ªteuses, stables entre capteurs h√©t√©rog√®nes et adapt√©es √† TinyML.
# 
# **Optionnels (am√©liorations futures)** : √©nergie de bandes fr√©quentielles (FFT), d√©riv√©es/jerk, corr√©lations inter-axes, entropie, zcr (zero-crossing rate), etc.
# 
# ---
# 
# ## 4) Variable cible (labels)
# Pas de colonne `label` dans les fichiers ‚Üí on utilise la **convention de nommage** :
# - `label = 1` si le nom du fichier contient : `earthquake`, `hitting_arm`, `hitting_platform`, `extra_weigth`, `magnet`.
# - `label = 0` sinon (fichiers IMU ¬´ normaux ¬ª : `IMU_2Hz`, `IMU_10Hz`, `IMU_Acc_Gyro_20Hz`, ‚Ä¶).
# 
# **Label de fen√™tre** (majorit√© simple) :  
# si ‚â• 50 % des points de la fen√™tre sont `1` ‚Üí fen√™tre `1`, sinon `0`.  
# C‚Äôest un compromis simple qui stabilise le bruit instantan√©.
# 
# ---
# 
# ## 5) Pr√©-traitements retenus
# - **Casting** num√©rique robuste (`errors='coerce'`), suppression des lignes enti√®rement vides sur les colonnes IMU.
# - **Standardisation par source** (fichier) pour absorber les offsets capteurs :
#   \[
#   z = \frac{x-\mu_{\text{source}}}{\sigma_{\text{source}}}
#   \]
#   Cette normalisation est apprise **s√©par√©ment** pour chaque fichier afin d‚Äô√©viter les fuites d‚Äôinformation.
# - **Agr√©gation fen√™tre** ‚Üí features statistiques (cf. ¬ß3).
# 
# ---
# 
# ## 6) Jeu d‚Äô√©valuation & fuite d‚Äôinformation
# Les √©chantillons d‚Äôune m√™me source (fichier) sont **fortement corr√©l√©s**. On doit donc √©valuer **par fichier**, pas par lignes al√©atoires.
# 
# - **Split** : `GroupShuffleSplit` avec `group = source` (le nom du fichier).  
#   Test ‚âà 30 % des fichiers, Train ‚âà 70 %.  
# - Variante robuste : **Leave-One-Group-Out** (LOGO) pour tester chaque fichier √† tour de r√¥le (plus co√ªteux mais plus informatif).
# 
# ---
# 
# ## 7) M√©triques d‚Äô√©valuation
# - **ROC-AUC** (global) pour mesurer la capacit√© de classement.  
# - **PR-AUC** / **F1** pour tenir compte du **d√©s√©quilibre** (anomalies rares).  
# - **Matrice de confusion** (TP/FP/FN/TN) pour fixer des seuils r√©alistes en exploitation (\(P(\text{FP})\) acceptable vs rappel cibl√©).
# 
# **Cible industrielle** (exemple) :  
# - Rappel (anomalies) ‚â• 95 % ;  
# - Taux de fausses alertes ‚â§ 2 % (√† ajuster selon le contexte).
# 
# ---
# 
# ## 8) Gestion du d√©s√©quilibre des classes
# - **Fen√™trage** et **majority-label** r√©duisent d√©j√† l‚Äôextr√™me raret√© point-par-point.  
# - Pendant l‚Äôentra√Ænement : `class_weight='balanced'` ou sur-/sous-√©chantillonnage des fen√™tres.  
# - √âvaluation : toujours **stratifier par fichier** et **rapporter PR-AUC**.
# 
# ---
# 
# ## 9) Mod√®le baseline (Step 4, annonc√© ici)
# - **RandomForest** (200 arbres) sur features statistiques ‚Üí robuste, peu de r√©glages, interpr√©table via l‚Äôimportance des variables.  
# - **Alternatives TinyML** :  
#   - **Logistic Regression** (faible empreinte m√©moire),  
#   - **Tiny tree/oblique tree** (1‚Äì4 n≈ìuds),  
#   - **Isolation Forest** (non supervis√©) si on veut √©viter les labels de nommage.
# 
# ---
# 
# ## 10) Contraintes temps r√©el & embarqu√©
# - **Latence** ‚âà taille de fen√™tre / fr√©quence (ex. 50 pts @ 20 Hz ‚Üí 2,5 s).  
# - Si la latence doit √™tre < 1 s, deux leviers :  
#   1) diminuer `W` (ex. 20‚Äì30) pour les fichiers 10/20 Hz,  
#   2) utiliser **fen√™tres chevauch√©es** (stride plus petit) pour densifier les d√©cisions sans augmenter la latence per√ßue.
# - **Co√ªt de calcul** : features statistiques \(O(W)\), mod√®le l√©ger (LR/mini-tree) pour MCU.
# 
# ---
# 
# ## 11) Crit√®res de r√©ussite (baseline)
# - Pipeline **reproductible** (m√™mes splits, m√™me normalisation par source).  
# - ROC-AUC ‚â• 0,90 et PR-AUC convenable sur le test par fichier.  
# - Courbes ROC/PR + matrice de confusion + top features importances.  
# - Sauvegarde du mod√®le et des param√®tres de normalisation (par source si n√©cessaire).
# 
# ---
# 
# ## 12) Limites & risques
# - **Labels d√©riv√©s du nom de fichier** ‚Üí approximatifs si des segments ¬´ mixtes ¬ª existent ; la majority-rule l‚Äôatt√©nue mais ne l‚Äô√©limine pas.  
# - **Fr√©quences h√©t√©rog√®nes** ‚Üí une fen√™tre fixe correspond √† des dur√©es diff√©rentes ; √† terme, utiliser une *dur√©e* (en secondes) plut√¥t qu‚Äôun nombre d‚Äô√©chantillons.  
# - **Covariate shift** entre t√¢ches (screwdriving vs painting) : valider par **LOGO** et, si besoin, ajouter des features sp√©cifiques par t√¢che (ou un domaine-label si disponible).
# 
# ---
# 
# ## 13) Repro (configuration de r√©f√©rence)
# 
# ```python
# CONFIG = {
#   "window_size": 50,
#   "window_overlap": 0.0,        # 0.5 recommand√© si besoin
#   "features": ["mean","std","min","max","rms"],
#   "signals": ["acc_norm","gyro_norm","mag_norm"],
#   "split": {"method":"GroupShuffleSplit", "test_size":0.3, "group":"source", "seed":42},
#   "model": {"type":"RandomForest", "n_estimators":200, "random_state":0, "n_jobs":-1},
#   "metrics": ["classification_report","confusion_matrix","roc_auc","pr_auc"]
# }
# 

# %% [markdown]
# # IV - Mod√®le baseline + √©valuation
# 

# %%
# S√©lection des features du mod√®le
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_auc_score,
    average_precision_score,
    RocCurveDisplay,
    PrecisionRecallDisplay,
    recall_score, 
    precision_score, 
    f1_score
)
from sklearn.model_selection import GroupShuffleSplit
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np # üí° AJOUT N√âCESSAIRE

feat_cols_model = [c for c in dataset.columns
                   if c not in ('label','source')]

X = dataset[feat_cols_model].values
y = dataset['label'].values
groups = dataset['source'].values

# Split par fichier
gss = GroupShuffleSplit(n_splits=1, test_size=0.3, random_state=42)
train_idx, test_idx = next(gss.split(X, y, groups))

Xtr, Xte = X[train_idx], X[test_idx]
ytr, yte = y[train_idx], y[test_idx]

# Baseline avec poids de classes √©quilibr√©s
clf = RandomForestClassifier(
    n_estimators=200,
    random_state=0,
    n_jobs=-1,
    class_weight="balanced"
)
clf.fit(Xtr, ytr)

# Obtenir les probabilit√©s de la classe 1
proba = clf.predict_proba(Xte)[:,1]


# =========================================================
# √âTAPE 1 : Analyse des seuils (tableau de compromis)
# =========================================================
thresholds_to_test = [0.1, 0.3, 0.5, 0.7, 0.9]
results = []

print("=== Analyse de la performance selon le seuil de d√©cision ===")

for T in thresholds_to_test:
    pred_T = (proba >= T).astype(int)

    recall = recall_score(yte, pred_T, zero_division=0)
    precision = precision_score(yte, pred_T, zero_division=0)
    
    # Calcul du Taux de Faux Positifs (FPR)
    cm = confusion_matrix(yte, pred_T)
    tn_plus_fp = cm[0, :].sum()
    fp_rate = cm[0, 1] / tn_plus_fp if tn_plus_fp > 0 else 0

    results.append({
        'Threshold': T,
        'Recall (Classe 1)': f'{recall:.3f}',
        'Precision (Classe 1)': f'{precision:.3f}',
        'F1-Score': f'{f1_score(yte, pred_T, zero_division=0):.3f}',
        'False Positive Rate (FPR)': f'{fp_rate:.4f}'
    })

results_df = pd.DataFrame(results)
print(results_df.to_markdown(index=False))


# =========================================================
# √âTAPE 2 : S√©lection du SEUIL OPTIMAL pour l'√©quilibre Rappel/FPR
# =========================================================
MAX_FPR = 0.05 # Contrainte industrielle : Taux de FP max 5%

best_metric = -1
selected_T = -1

# Recherche sur une grille plus fine
thresholds_fine_grid = [i/100 for i in range(5, 95, 5)] 

for T in thresholds_fine_grid:
    pred_T = (proba >= T).astype(int)
    
    # N√©cessaire pour √©viter les erreurs de calcul si une seule classe est pr√©dite
    if len(np.unique(pred_T)) < 2: 
        continue
        
    current_recall = recall_score(yte, pred_T, zero_division=0)
    cm = confusion_matrix(yte, pred_T)
    tn_plus_fp = cm[0, :].sum()
    current_fp_rate = cm[0, 1] / tn_plus_fp if tn_plus_fp > 0 else 1.0

    # On maximise le Rappel tout en respectant la contrainte MAX_FPR
    if current_fp_rate <= MAX_FPR and current_recall > best_metric:
        best_metric = current_recall
        selected_T = T

if selected_T != -1:
    OPTIMAL_THRESHOLD = selected_T
    print(f"\nSeuil s√©lectionn√© pour maximiser le Rappel (contrainte FPR < {MAX_FPR:.2f}) : {OPTIMAL_THRESHOLD:.2f}")
else:
    # Si la contrainte FPR est trop stricte, on utilise le seuil 0.5 comme fallback
    OPTIMAL_THRESHOLD = 0.5
    print(f"\nAttention: Aucun seuil ne respecte FPR < {MAX_FPR:.2f}. Utilisation du seuil par d√©faut {OPTIMAL_THRESHOLD}.")

# =========================================================
# √âTAPE 3 : √âvaluation finale avec le seuil optimal
# =========================================================
pred_optimal = (proba >= OPTIMAL_THRESHOLD).astype(int)

print(f"\n=== Rapport classification (test, Seuil {OPTIMAL_THRESHOLD:.2f}) ===")
print(classification_report(yte, pred_optimal, digits=3))

print(f"=== Matrice de confusion (Seuil {OPTIMAL_THRESHOLD:.2f}) ===")
print(confusion_matrix(yte, pred_optimal))


# ROC-AUC / PR-AUC
try:
    roc = roc_auc_score(yte, proba)
    pr 	= average_precision_score(yte, proba)
    print(f"\nROC-AUC : {roc:.3f}")
    print(f"PR-AUC 	: {pr:.3f}")

    RocCurveDisplay.from_predictions(yte, proba)
    plt.title("ROC ‚Äî RandomForest baseline")
    plt.show()

    PrecisionRecallDisplay.from_predictions(yte, proba)
    plt.title("Precision‚ÄìRecall ‚Äî RandomForest baseline")
    plt.show()
except ValueError as e:
    print("Courbes ROC/PR non calculables :", e)

# Top features
imp = pd.Series(clf.feature_importances_, index=feat_cols_model).sort_values(ascending=False).head(15)
plt.figure()
imp.plot(kind="barh")
plt.gca().invert_yaxis()
plt.title("Top feature importances ‚Äî RandomForest")
plt.xlabel("importance")
plt.show()

# %% [markdown]
# # V - Grid Search + Ensembles (LAB 6 appliqu√© au projet)

# %%
from sklearn.model_selection import GroupShuffleSplit, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import (
    accuracy_score, f1_score, precision_score, recall_score,
    confusion_matrix, classification_report,
    roc_auc_score, average_precision_score
)
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


# %% [markdown]
# ## 1) Reprise des features / labels / groupes 

# %%
feat_cols_model = [c for c in dataset.columns
                   if c not in ("label", "source")]

X = dataset[feat_cols_model].values
y = dataset["label"].values
groups = dataset["source"].values

# Split "externe" par fichier : train vs test 
gss_outer = GroupShuffleSplit(n_splits=1, test_size=0.3, random_state=42)
train_idx, test_idx = next(gss_outer.split(X, y, groups))

Xtr, Xte = X[train_idx], X[test_idx]
ytr, yte = y[train_idx], y[test_idx]
groups_tr = groups[train_idx]

print("Train size :", Xtr.shape, " Test size :", Xte.shape)

# %% [markdown]
# ## 2) GridSearch sur RandomForest (baseline optimis√©e)

# %%
rf_base = RandomForestClassifier(
    random_state=0,
    n_jobs=-1,
    class_weight="balanced"   # important avec le d√©s√©quilibre de classes
)

rf_param_grid = {
    "n_estimators": [100, 200, 400],
    "max_depth": [None, 10, 20],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4],
    "max_features": ["sqrt", "log2"]
}

# CV interne par fichier aussi (GroupShuffleSplit)
cv_inner = GroupShuffleSplit(n_splits=3, test_size=0.2, random_state=42)

rf_grid = GridSearchCV(
    estimator=rf_base,
    param_grid=rf_param_grid,
    cv=cv_inner,
    scoring="f1",      # meilleur que accuracy sur ce type de probl√®me
    n_jobs=-1,
    verbose=2
)

rf_grid.fit(Xtr, ytr, groups=groups_tr)

print("\n=== RandomForest ‚Äî meilleurs hyperparam√®tres (CV interne) ===")
print(rf_grid.best_params_)
print(f"Best CV F1-score: {rf_grid.best_score_:.3f}")

best_rf = rf_grid.best_estimator_

# %% [markdown]
# ## 3) Helper d'√©valuation pour tous les mod√®les

# %%


def eval_model(name, model, Xtr, ytr, Xte, yte):
    """Fit + pr√©diction + m√©triques + temps, pour un mod√®le donn√©."""
    t0 = time.time()
    model.fit(Xtr, ytr)
    fit_time = time.time() - t0

    t1 = time.time()
    y_pred = model.predict(Xte)
    infer_time = time.time() - t1

    # Probabilit√©s si dispo (sinon None)
    try:
        y_proba = model.predict_proba(Xte)[:, 1]
    except AttributeError:
        y_proba = None

    acc  = accuracy_score(yte, y_pred)
    f1   = f1_score(yte, y_pred)
    prec = precision_score(yte, y_pred)
    rec  = recall_score(yte, y_pred)

    print(f"\n================ {name} ================")
    print(classification_report(yte, y_pred, digits=3))
    print("Confusion matrix:\n", confusion_matrix(yte, y_pred))

    roc = pr = None
    if y_proba is not None and len(np.unique(yte)) == 2:
        roc = roc_auc_score(yte, y_proba)
        pr  = average_precision_score(yte, y_proba)
        print(f"ROC-AUC: {roc:.3f}")
        print(f"PR-AUC : {pr:.3f}")

    return {
        "model": name,
        "accuracy": acc,
        "f1": f1,
        "precision": prec,
        "recall": rec,
        "roc_auc": roc,
        "pr_auc": pr,
        "fit_time_s": fit_time,
        "infer_time_s": infer_time
    }

results = []

# √âvaluation de la baseline optimis√©e RandomForest
results.append(eval_model("RandomForest (tuned)", best_rf, Xtr, ytr, Xte, yte))


# %% [markdown]
# ## 4) Bagging SVM

# %%
# =========================================================
# üí° N√âCESSAIRE : Nouvelle d√©finition de eval_model avec seuil
# (Assurez-vous d'ex√©cuter ce bloc AVANT d'ex√©cuter les ensembles)
# =========================================================
def eval_model(name, model, Xtr, ytr, Xte, yte, threshold=0.5):
    """Fit + pr√©diction + m√©triques + temps, pour un mod√®le donn√©, avec ajustement du seuil."""
    t0 = time.time()
    model.fit(Xtr, ytr)
    fit_time = time.time() - t0

    t1 = time.time()
    
    y_proba = None
    try:
        # Tente d'utiliser predict_proba pour l'ajustement du seuil
        y_proba = model.predict_proba(Xte)[:, 1]
        
        # üéØ Application du seuil personnalis√©
        y_pred = (y_proba >= threshold).astype(int)
        
        infer_time = time.time() - t1
        
    except AttributeError:
        # Si predict_proba n'est pas disponible (ex: certains mod√®les SVM sans probability=True)
        y_pred = model.predict(Xte)
        infer_time = time.time() - t1
    
    # M√©triques
    acc  = accuracy_score(yte, y_pred)
    f1   = f1_score(yte, y_pred, zero_division=0)
    prec = precision_score(yte, y_pred, zero_division=0)
    rec  = recall_score(yte, y_pred, zero_division=0)

    print(f"\n================ {name} (Threshold={threshold:.2f}) ================")
    print(classification_report(yte, y_pred, digits=3))
    print("Confusion matrix:\n", confusion_matrix(yte, y_pred))

    roc = pr = None
    if y_proba is not None and len(np.unique(yte)) == 2:
        try:
            roc = roc_auc_score(yte, y_proba)
            pr  = average_precision_score(yte, y_proba)
            print(f"ROC-AUC: {roc:.3f}")
            print(f"PR-AUC : {pr:.3f}")
        except ValueError:
             print("ROC/PR non calculables.")


    return {
        "model": name,
        "threshold": threshold, # Ajout du seuil aux r√©sultats
        "accuracy": acc,
        "f1": f1,
        "precision": prec,
        "recall": rec,
        "roc_auc": roc,
        "pr_auc": pr,
        "fit_time_s": fit_time,
        "infer_time_s": infer_time
    }
# Fin de la nouvelle d√©finition de eval_model

# =========================================================
# APPLICATION DU SEUIL √âLEV√â (0.95 par exemple)
# =========================================================
# Utilisez le seuil que vous avez trouv√© optimal pour minimiser les FP avec RF (par ex. 0.95)
OPTIMAL_THRESHOLD_ENSEMBLES = 0.95 

# --- Ensemble 1 : Bagging SVM ( ---

# Assurez-vous que svm_base a bien probability=True (c'est le cas dans votre code)
svm_base = SVC(
    probability=True,
    random_state=42,
    class_weight="balanced",
    C=1.0,
    kernel="rbf",
    gamma="scale"
)

bag_svm = BaggingClassifier(
    estimator=svm_base,
    n_estimators=5,       # ne pas mettre trop haut pour ne pas tuer le PC
    max_samples=0.7,
    n_jobs=-1,
    random_state=42
)

results.append(
    # üéØ APPLICATION DU SEUIL ICI
    eval_model("Bagging SVM (Pr√©cision focus)", bag_svm, Xtr, ytr, Xte, yte, threshold=OPTIMAL_THRESHOLD_ENSEMBLES)
)


# --- Ensemble 2 : Bagging Decision Tree ---

tree_base = DecisionTreeClassifier(
    random_state=42,
    class_weight="balanced",
    max_depth=5,
    min_samples_split=5,
    min_samples_leaf=2
)

bag_tree = BaggingClassifier(
    estimator=tree_base,
    n_estimators=15,      # ne pas mettre trop haut pour ne pas tuer le PC
    max_samples=0.7,
    n_jobs=-1,
    random_state=42
)

results.append(
    # üéØ APPLICATION DU SEUIL ICI
    eval_model("Bagging Tree (Pr√©cision focus)", bag_tree, Xtr, ytr, Xte, yte, threshold=OPTIMAL_THRESHOLD_ENSEMBLES)
)


# --- Ensemble 3 : Voting (RF + Bagging SVM + Bagging Tree) ---

# Note : best_rf doit avoir √©t√© d√©fini par le GridSearchCV pr√©c√©dent
voting_clf = VotingClassifier(
    estimators=[
        ("rf", best_rf),           
        ("bag_svm", bag_svm),
        ("bag_tree", bag_tree)
    ],
    voting="soft",
    n_jobs=-1
)

results.append(
    # APPLICATION DU SEUIL ICI
    eval_model("Voting (Pr√©cision focus)", voting_clf, Xtr, ytr, Xte, yte, threshold=OPTIMAL_THRESHOLD_ENSEMBLES)
)


# --- Tableau r√©cap + barplot ---

results_df = pd.DataFrame(results)
display(results_df.sort_values("f1", ascending=False))

plt.figure(figsize=(8, 4))
plt.bar(results_df["model"], results_df["accuracy"])
plt.xticks(rotation=20, ha="right")
plt.ylabel("Accuracy (test)")
plt.title("Comparison of models on IMU anomaly detection")
plt.tight_layout()
plt.show()
